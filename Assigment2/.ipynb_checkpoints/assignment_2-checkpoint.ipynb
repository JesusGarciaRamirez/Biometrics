{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](logo.png)\n",
    "\n",
    "# Biometrics System Concepts\n",
    "\n",
    "# Assignment 1: Fingerprint Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and test a key-point based fingerprint recognition/verification system.\n",
    "\n",
    "A high-level description is provided with links to or hints of code snippets and libraries that you can reuse/adapt at your will (with proper referencing!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps need to be implemented:\n",
    "1. Importing the required packages\n",
    "2. Reading image data, conversion to gray-scale\n",
    "3. Preprocessing: Enhancement using orientation/frequency filtering and Segmentation of foreground \n",
    "4. \"Minutiae\" detection using keypoint detectors and descriptors from OpenCV\n",
    "5. Matching using keypoint matching\n",
    "6. **Validation using a verification and identification scenario** (cf. previous assignment)\n",
    "\n",
    "For steps 1-5, code examples will be provided below. You can and are invited to adapt these at your will (different parameter settings, different choices of alogorithmic components). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV package\n",
    "import cv2\n",
    "# standard array processing package\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "#to plot the figures inside the notebook:\n",
    "%matplotlib inline\n",
    "# file path processing package\n",
    "from pathlib import Path\n",
    "# local modules for fingerprint enhancement\n",
    "import fprmodules.enhancement as fe\n",
    "# package for some simple biometric metrics. \n",
    "# Of course you can use the code you have developed in the previous assignment\n",
    "# import pyeer\n",
    "from sklearn.metrics import roc_curve\n",
    "# Pickle allows to save and read intermediate results (similar to save and load in Matlab)\n",
    "import pickle\n",
    "# a visual progress bar library\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reading image data and conversion to gray scale\n",
    "\n",
    "We will make use of the Fingerprint Verification Competition'2002 Database ([FVC'2002](http://bias.csr.unibo.it/fvc2002/default.asp)) images. This was the second in a series of benchmark tests that were made publically available. While we do not have access to the full (880 images) database, you can test your algorithm on a subset of 80 images (10 individuals/fingers, 8 fingerprints per finger). Eventually, you can compare your results to the results that other competitors have uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a whole database, stored in a single folder with filenames as XXY.tig with XX the individual number and Y the fingerprint number for this individual\n",
    "# returns a list of images and a list of associated identity labels\n",
    "\n",
    "def read_DB(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    imagePaths = sorted(Path(path).rglob(\"*.tif\"))\n",
    "#     for imagePath in imagePaths:\n",
    "    for imagePath in tqdm_notebook(imagePaths):\n",
    "        image = cv2.imread(path + imagePath.name)\n",
    "        if (len(image.shape) > 2):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images.append(image)\n",
    "        label = imagePath.stem[1:3]\n",
    "        labels.append(label)\n",
    "    return (images, labels)\n",
    "\n",
    "# select a Database\n",
    "images_db1, labels_db1 = read_DB('./fprdata/FVC2002/DB1_B/')\n",
    "# images_db2, labels_db2 = read_DB('./fprdata/FVC2002/DB2_B/')\n",
    "# images_db3, labels_db3 = read_DB('./fprdata/FVC2002/DB3_B/')\n",
    "# images_db4, labels_db4 = read_DB('./fprdata/FVC2002/DB4_B/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify by showing some images\n",
    "print(images_db1[0].shape)\n",
    "plt.imshow(images_db1[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing: enhancement using ridge orientation/frequency estimation and segmentation of foreground\n",
    "\n",
    "We will re-use code developed by [Utkarsh Deshmukh](https://github.com/Utkarsh-Deshmukh/Fingerprint-Enhancement-Python), which is a Python-recode of Matlab-code developed by [Peter Kovesi](https://www.peterkovesi.com/matlabfns/index.html#fingerprints). I have slightly adapted this code to make it more compliant with OpenCV. \n",
    "\n",
    "It uses gradient-based orientation and local frequency estimation as input to an orientation and frequency selective filtering using a Gabor filterbank. \n",
    "\n",
    "At the same time a foreground region segmentation (mask) is determined by calculating the standard deviation in local windows and thresholding above a certain level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the enhanced images and the associated segmentation masks\n",
    "\n",
    "def enhance_images(images):\n",
    "\n",
    "    images_e_u = []\n",
    "    masks = []\n",
    "#     for i, image in enumerate(tqdm_notebook(images)):\n",
    "    for i, image in tqdm(enumerate(images)):\n",
    "\n",
    "        try:\n",
    "            # Gabor filtering\n",
    "            img_e, mask, orientim, freqim = fe.image_enhance(image)\n",
    "            # Normalize in the [0,255] range\n",
    "            img_e_u = cv2.normalize(img_e, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=0)\n",
    "            images_e_u.append(img_e_u)\n",
    "        except:\n",
    "            print('error for: ', i)\n",
    "            images_e_u.append(image)\n",
    "        masks.append(mask)\n",
    "    return images_e_u, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_enhanced_db1, masks_db1 = enhance_images(images_db1)\n",
    "# # images_db2 = enhance_images(images_db2)\n",
    "# # images_db3 = enhance_images(images_db3)\n",
    "# # images_db4 = enhance_images(images_db4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intermediate (computation heavy) results can be saved on file using the pickle package\n",
    "# p_file = open( \"./fprdata/DB1_enhanced.p\", \"wb\" )\n",
    "# pickle.dump([images_enhanced_db1, labels_db1, masks_db1], p_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and read back for further processing\n",
    "p_file = open( \"./fprdata/DB1_enhanced.p\", \"rb\" )\n",
    "[images_enhanced_db1, labels_db1, masks_db1] = pickle.load(p_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by showing some input and processed images\n",
    "\n",
    "ax = plt.figure()\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 3, figsize=(10,10))\n",
    "ax[0,0].imshow(images_db1[0], cmap='gray')\n",
    "ax[0,1].imshow(images_enhanced_db1[0], cmap='gray')\n",
    "ax[0,2].imshow(images_enhanced_db1[0]*masks_db1[0], cmap='gray')\n",
    "\n",
    "ax[1,0].imshow(images_db1[1], cmap='gray')\n",
    "ax[1,1].imshow(images_enhanced_db1[1], cmap='gray')\n",
    "ax[1,2].imshow(images_enhanced_db1[1]*masks_db1[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Keypoint Extraction and matching\n",
    "The basis/traditional fingerprint matching algorithm is based on matching keypoints, called minutiae. They are defined as the ridge endings and bifurcations and can be determined by first thresholding the FP image and skeletonizing/thinning the enhanced image. For those really interested (but not part of this assignment) in experimenting with such an approach, some code is provided in the fprmodules/minutiae_extraction folder.\n",
    "\n",
    "Instead, we will rely on establised methods developed in Computer Vision, where the use of keypoint features has proven to be very succesfull (prior to the advent of Deep Learning) in image classification, image alignment and object recognition tasks.\n",
    "\n",
    "In this assignment, you are encouraged to try out a series of detection/description/matching algorithms. A short overview of these methods can be found in the OpenCV documentation: [keypoint_detection_descrption_matching](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html#py-table-of-content-feature2d).\n",
    "Please read these pages carefully and try to understand these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Keypoint detection and feature description\n",
    "\n",
    "Below, you find one example, that uses ORB features. You can experiment with other feature descriptors and [matching routines](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html#matcher) as presented on these pages.\n",
    "\n",
    "In this template code, we detect and describe keypoints and features. You can test the whole procedure for different detectors. We expect that the keypoints will be detected at \"interesting\" locations, where locally things seem to change drastically (endings, bifurcations, cores, deltas ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ORB keypoint detector and descriptor\n",
    "# this can be supplemented with other detectors\n",
    "\n",
    "def detect_ORB(image, max_features = 500):\n",
    "    orb = cv2.ORB_create(max_features)\n",
    "    kp1, desc1 = orb.detectAndCompute(image, None)\n",
    "    return kp1, desc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many false keypoints will be generated at the edge of the foreground mask, since ridges seem to terminate due to the clipping.\n",
    "# we remove these by a morpholigical erosion (shrinking) of the foreground mask and deleting the keypoints outside.\n",
    "\n",
    "def remove_edge_kps(mask, kp, desc):\n",
    "    mask_b = mask.astype(np.uint8)  #convert to an unsigned byte\n",
    "    # morphological erosion\n",
    "    mask_b*=255\n",
    "    mask_e = cv2.erode(mask_b, kernel = np.ones((5,5),np.uint8), iterations = 5)\n",
    "    # remove keypoints and their descriptors that lie outside this eroded mask\n",
    "    kpn = [kp[i] for i in range(len(kp)) if mask_e.item(int(kp[i].pt[1]),int(kp[i].pt[0])) == 255]\n",
    "    descn = np.vstack([desc[i] for i in range(len(kp)) if mask_e.item(int(kp[i].pt[1]),int(kp[i].pt[0])) == 255])\n",
    "    return kpn, descn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example\n",
    "\n",
    "testNr1 = 0\n",
    "testNr2 = 7\n",
    "# find the keypoints and descriptors with ORB for two images of the same finger\n",
    "\n",
    "kp1, des1 = detect_ORB(images_enhanced_db1[testNr1], max_features = 500)\n",
    "kp1, des1 = remove_edge_kps(masks_db1[testNr1], kp1, des1)\n",
    "\n",
    "kp2, des2 = detect_ORB(images_enhanced_db1[testNr2], max_features = 500)\n",
    "kp2, des2 = remove_edge_kps(masks_db1[testNr2], kp2, des2)\n",
    "\n",
    "# plot the keypoints and inspect how genuine they are and if there is a subset that seems to match left and right\n",
    "show_img1 = cv2.drawKeypoints(images_enhanced_db1[testNr1], kp1, None, (255, 0, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_img2 = cv2.drawKeypoints(images_enhanced_db1[testNr2], kp2, None, (255, 0, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "\n",
    "ax = plt.figure()\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(10,10))\n",
    "ax[0].imshow(show_img1)\n",
    "ax[1].imshow(show_img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Local Matching by feature vector comparison\n",
    "\n",
    "Once all images have been processed we can start matching the keypoints between the images. \n",
    "\n",
    "We can do this on a purely **local level** by comparing the feature decriptors in one image with the feature descriptors in the other image. Note that these feature descriptors give a vectorial summary of the neighbourhood around each keypoint. A simple metric on these vectors (Euclidean Distance for continuous variables, Hamming Distance for binary variables) can then be used to determine similarity. \n",
    "\n",
    "A simple Brute Force matching strategy is to measure these distances between every possible pair of keypoints (descriptors) in both images. We can then define simple scalar measures on this set of distances, such as the number of pairs with a distance smaller than a set threshold, or the sum/mean of the first N distances (ranked from small to larger), etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute Force matcher on a pair of KeyPoint sets using the local descriptor for similarity\n",
    "\n",
    "def match_BruteForce_local(image1, image2, des1, des2):\n",
    "        \n",
    "    # Brute Force all pair matcher: returns all pairs of best matches\n",
    "    # depending on type of descriptor use the corresponding norm\n",
    "    # crossCheck=True only retains pairs of keypoints that are each other best matching pair\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    # sort matches based on feature distance\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "matches = match_BruteForce_local(images_enhanced_db1[testNr1], images_enhanced_db1[testNr2], des1, des2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the result using drawMatches\n",
    "\n",
    "imMatches = cv2.drawMatches(images_enhanced_db1[testNr1],kp1,\n",
    "                            images_enhanced_db1[testNr2],kp2,matches, None) \n",
    "plt.imshow(imMatches),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Global matching by testing geometrical consistency\n",
    "\n",
    "[Here](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/) you can find a decsription and code how to start from the brute force matching results and estimate the best transformation (from a family of transformations) that aligns the two images. In the example given [here](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/), a homography-type transformation is searched for. However, this has too many degrees of freedom for our application. We substituted this by a more constrained (only 4 degrees of freedom) similarity (partial affine) transformation.\n",
    "These routines iteratively determine the minimal set of matching points that define a transformation that optimally aligns all other points as well, taking care of outliers at the same time. This method is a very general optimization technique and is called RANSAC, for \"RANdom SAmple Consensus\". See, apart from many other sources on the internet, [this presentation](http://www.cse.psu.edu/~rtc12/CSE486/lecture15.pdf) for further explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_BruteForce_global(image1, image2, kp1, kp2, matches, good_match_percent = 0.75):\n",
    "    \n",
    "    # a local function required to transform key_points by a certain transformation matrix\n",
    "    def transform_keypoints(key_points, transformation_matrix):\n",
    "        # convert keypoint list to Nx1x2 matrix\n",
    "        mat_points = cv2.KeyPoint.convert(key_points).reshape(-1,1,2)\n",
    "        # transform points \n",
    "        mat_reg_points = cv2.transform(mat_points, transformation_matrix)\n",
    "        # return transformed keypoint list\n",
    "        return cv2.KeyPoint.convert(mat_reg_points)    \n",
    "\n",
    "    # select the best x percent best matches (on local feature vector level) for further global comparison\n",
    "    GOOD_MATCH_PERCENT = good_match_percent\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    good_matches = matches[:numGoodMatches]\n",
    "   \n",
    "    # retain only the keypoints associated to the best matches \n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # estimate an optimal 2D affine transformation with 4 degrees of freedom,\n",
    "    # limited to combinations of translation, rotation, and uniform scaling\n",
    "    \n",
    "    # this is the core of the global consistency check: if we find the correct transformation\n",
    "    # (which we expect for genuine pairs and not for imposter pairs), we can use it as an\n",
    "    # additional check by verifying the geometrical quality of the match\n",
    "    \n",
    "    # M stores the optimal transformation\n",
    "    # inliers stores the indices of the subset of points that were finally used to calculate the optimal transformation\n",
    "    \n",
    "    M, inliers = cv2.estimateAffinePartial2D(src_pts, dst_pts, method =  cv2.RANSAC, \n",
    "                                             confidence = 0.9, ransacReprojThreshold = 10.0, \n",
    "                                             maxIters = 5000, refineIters = 10)\n",
    "\n",
    "    # get the inlier matches\n",
    "    matched = [x for x,y in zip(good_matches, inliers) if y[0] == 1]\n",
    "\n",
    "    # The optimal transformation is only correct for genuine pairs in about 75% of cases (experimentally on dataset DB1).\n",
    "    # One can build additional checks about the validity of the transformation,\n",
    "    # e.g. too large translations, rotations and/or scale factors\n",
    "    \n",
    "    # A simple one is to test the number of keypoints that were used in calculating the transformation. \n",
    "    # If this number is is too small, then the transformation is most possibly unreliable. \n",
    "    # In that case, we reset the transformation to the identity\n",
    "    if np.sum(inliers) < 5:\n",
    "        M = np.eye(2, 3, dtype=np.float32)\n",
    "\n",
    "    # transform the first keypoint set using the transformation M\n",
    "    kp1_reg = transform_keypoints(kp1, M)\n",
    "                                              \n",
    "    return kp1_reg, matched, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1_reg, matched, M = match_BruteForce_global(images_enhanced_db1[testNr1],\n",
    "                                              images_enhanced_db1[testNr2],\n",
    "                                              kp1,kp2, matches, good_match_percent = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the matches after affine transformation\n",
    "height, width = images_enhanced_db1[testNr2].shape[:2]\n",
    "im1Reg = cv2.warpAffine(images_enhanced_db1[testNr1], M, (width, height))\n",
    "\n",
    "# only the inlier matches (matched) are shown\n",
    "imMatches = cv2.drawMatches(im1Reg, kp1_reg, images_enhanced_db1[testNr2], kp2, matched, None) #, flags=2)\n",
    "plt.imshow(imMatches), plt.show()\n",
    "\n",
    "# Note; the hypothesis is that the transformation will be most of the time OK for genuine\n",
    "# pairs, but wrong for imposter pairs. \n",
    "\n",
    "# show all keypoints after geometric matching\n",
    "\n",
    "print(\"Affine_#inliers:{}\".format(len(matched)))\n",
    "show_img1 = cv2.drawKeypoints(im1Reg, kp1_reg, None, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_img2 = cv2.drawKeypoints(images_enhanced_db1[testNr2], kp2, None, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "\n",
    "ax = plt.figure()\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(10,10))\n",
    "ax[0].imshow(show_img1)\n",
    "ax[1].imshow(show_img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASKS\n",
    "\n",
    "Implement the following tasks for one of the 4 databases. I suggest either DB1 or DB2, since they seem a little easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue> Task 1 Test different Keypoint Detectors/Descriptors\n",
    "\n",
    "* OpenCV provides different KeyPoint detectors and descriptors (ORB, SIFT, SURF, BRIEF, ...). Briefly test, visually, which of these seem to extract relatively reliable interesting points from the fingerprints.\n",
    "* Note that for some of the KeyPoint descriptors, you need to have a special licence or a full implementation. Skip if you do not have it.\n",
    "* You also may have to tune the parameters of these detectors.\n",
    "* But don't spend too much time on this testing step. \n",
    "* If you do have the time and the computational resources, you can compare not just visually but by calculating all through using ROC and CMC curves as evidence (Cf. task 3)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue> Task 2. Determine a matching function\n",
    "    \n",
    "You have been given code to calculate matches on a local level and to calculate an affine transformation that allows a more global consistency check. \n",
    "    \n",
    "Define and test different matching functions that make use of these two-level features.\n",
    "    \n",
    "1. a function based only on the local features (e.g. number of pairs with feature distance below a set distance threshold, the sum or mean of the feature distance of the best N pairs, or any other function you think is a good candidate)\n",
    "2. a function based on the geometrical distances between pairs of KeyPoints are affine transformation. For instance, you can take again a brute force matcher that, instead of pairwise feature descriptor distances, computes pairwise geometrical distances between the two point sets. From there on you can then define different measures (number of below a threshold or sum/mean of distances, ...). You can also limit the number of point pairs to apply this to to the matching pairs that result from the local match and/or the ones that are returned as 'matched' by the Ransac procedure. \n",
    "3. any combination of local and global match metrics\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>  Task 3 Test complete system for authentication and identification scenario's\n",
    "\n",
    "* Choose (at least) one of the databases provided\n",
    "* calculate the ROC (false match to false non match rate) for an authentication scenario \n",
    "* and a Cumulative Match Characteristic (CMC) curve, which plots the rank (order in the candidate list) on the x-axis and the probability of identification at that or better rank on the y-axis. Feel free to reuse your code of the previous assignment!\n",
    "\n",
    "* Of course you can calulate and store the features upfront before starting the validation procedures.\n",
    "* Also follow the pipeline and the datastructure of the first assignment on validation, where you calculated genuine and imposter scores (cf. the genuine_id and scores lists that you generated)\n",
    "    \n",
    "    \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>  Task 4 Discuss your results\n",
    "    \n",
    "* which parts are critical?\n",
    "* given more time, how would you improve?\n",
    "* compare your results to the results that are available [online](http://bias.csr.unibo.it/fvc2002/results.asp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
